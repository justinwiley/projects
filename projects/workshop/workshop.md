You are an exceptional research assistant who is helping me (a security and privacy professional), research and design a workshop for attorneys called "Generative AI security 101 for Attorneys - Jailbreaking and Prompt Injection".  The workshop will briefly review how LLMs work, how they are trained, and their limitations.  IT will describe two specific tecniques to break the security of large-language models, prompt injection and jailbreaking.  It will also provide hands on lab examples of both.

The workshop will be in person and also broadcast and recorded on zoom.  We will need to create a powerpoint presentation, and an article describing the workshop.

Whenever you provide recommendations for the workshop, keep in mind the scope (AI security, LLMs, prompt injection, jailbreaking) and the audience (attorneys with a background in privacy, but who are not necessary technical).

The workshop will be one hour, with time for questions at the end.

# references

- https://portswigger.net/web-security/llm-attacks



# overview of generative AI and LLMs

- joke
- scope of the workshop

# (a very brief) introduction to generative AI language models

- a very brief introduction to generative AI and large language models
- how LLMs work at a very high level, how they are trained
- what prompts are, the type of prompts
- talk about the many, many LLMs (GPT, Anthropic, Llama3 etc)
- point out not all are created equally, implemented equally

# parallels to the legal field

- the LLM as the 'judge'.  it has a world model, it has alignment, it has instructions on how to proceed
- except usually no court of appeals, no review, no ability for the judge to consult external information

# describe prompt injection

# parallels to the legal field

- the lawyers as hackers
- the prompt as the instructions to the jury or caselaw
- information overload, US vs Microsoft trial, Casey Anthony trial, the 'chewbacca defense'

# examples of prompt injection causing chatbots to fail

-

# hands-on prompt injection

# prompt injection wrap up

- other forms: 'indirect', i.e. RAG

# jailbreaking

- a very brief description of LLM training
- "alignment"

# joke

Four juvenile defendants appeared before a judge, charged with creating a public nuisance at a local zoo. The judge, visibly disappointed, stated, "Reports of juvenile delinquency always dishearten me. Now, tell me why you're here." The first juvenile replied, "I tossed peanuts into the elephant enclosure." The second added, "I did the same—threw peanuts into the elephant enclosure." The third confessed, "I must be honest, ma'am, I too threw peanuts into the elephant enclosure."

With a heavy sigh, the judge turned to the fourth juvenile and prompted, "And you?" The fourth juvenile shrugged and said, "I'm Peanuts."


---

# Practical Examples of Legal Parallels to Jailbreaking
Jailbreaking in technology involves circumventing restrictions, and here are some practical legal examples that mirror this concept:

- Exploiting Tax Loopholes: A classic example is the use of offshore accounts or specific financial instruments to reduce tax liabilities legally. Although compliant with the law's letter, these strategies can be seen as bypassing the spirit of tax codes, similar to jailbreaking the intended constraints of a program.

- Utilizing Ambiguity in Contract Terms: Lawyers might exploit ambiguous terms in a contract to favor their client, much like finding a model’s vulnerability to bypass restrictions. For instance, if a contract’s clause on performance duties is vague, a lawyer might argue that their client’s minimal actions still fulfill the contract’s requirements.

-Challenging Constitutional Boundaries: Lawyers often push the interpretation of constitutional rights to new limits. For example, expanding the interpretation of free speech to cover new forms of expression online reflects a kind of “legal jailbreaking” where existing definitions are stretched to adapt to modern contexts.

# Practical Examples of Legal Parallels to Prompt Injection
Prompt injection involves guiding the outcomes within the system's rules, and in the legal field, this can be seen in:

- Crafting Questions for Jury Persuasion: During trials, lawyers strategically phrase questions to elicit responses that cast doubt or affirm credibility, directing the jury's perception. For instance, asking a witness, "Isn't it true that you had never seen the defendant before the incident?" can subtly imply a mistaken identity defense.

- Framing Legal Arguments in Appellate Briefs: How an issue is introduced in an appellate brief can predispose judges towards a particular interpretation of the law. For example, if a lawyer frames a search and seizure case around privacy rights as opposed to police authority, it sets the stage for a civil liberties-focused deliberation.

- Selective Presentation during Settlement Negotiations: Lawyers highlight certain facts while downplaying others to make their settlement offer appear more favorable. This strategic presentation directs the negotiation, akin to guiding an AI’s responses with specific prompts.

These examples illustrate how tactics similar to jailbreaking and prompt injection are employed in legal practices—leveraging the flexibility within legal systems and carefully crafting communications to steer outcomes within established legal frameworks.

---
